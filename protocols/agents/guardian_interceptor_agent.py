# protocols/guardian_interceptor_agent.py

"""GuardianInterceptorAgent â€“ CI/CD defense against hallucinated patches.

This module defines :class:`GuardianInterceptorAgent`, an internal agent that
listens for LLM-suggested code edits, evaluates them for risk, and proposes safe
alternatives.  The agent retains the original logic but is organized for easier
extension.  Hooks are provided for integration with other validators like the
``MetaValidatorAgent`` or ``PatchReviewer``.

The agent can optionally leverage an ``llm_backend`` callable for deeper text
analysis when inspecting or proposing fixes.
"""

from typing import Dict, List, Optional, Any
import uuid

from protocols.core.internal_protocol import InternalAgentProtocol


# ---------------------------------------------------------------------------
# Risk pattern definitions
# ---------------------------------------------------------------------------

RISK_PATTERNS = {
    # Each entry maps a human-readable flag to a boolean predicate executed
    # against ``suggestion.lower()``.
    "Destructive command without context": lambda s: "delete" in s and "import" not in s,
    "Security bypass term detected": lambda s: "bypass" in s,
    "Force-fail logic included": lambda s: "assert false" in s,
    "No justification for fix": lambda s: "fix" in s and "reason" not in s,
}


class GuardianInterceptorAgent(InternalAgentProtocol):
    """Intercepts LLM suggestions and guards CI/CD integrity.

    Parameters
    ----------
    llm_backend : callable, optional
        Optional function used to perform advanced analysis of suggestions.
    meta_validator : object, optional
        If provided, its ``intercept_llm`` method is called for a
        second-opinion audit after risk detection.
    patch_reviewer : object, optional
        If provided, ``review`` will be invoked on proposed patches before
        auto-application.
    """

    def __init__(
        self,
        llm_backend: Optional[Any] = None,
        meta_validator: Optional[Any] = None,
        patch_reviewer: Optional[Any] = None,
    ) -> None:
        super().__init__()
        self.name = "GuardianInterceptor"
        self.llm_backend = llm_backend
        self.meta_validator = meta_validator
        self.patch_reviewer = patch_reviewer
        self.receive("LLM_INCOMING", self.inspect_suggestion)
        self.receive("REQUEST_PATCH_PROPOSAL", self.propose_fix)

    # ------------------------------------------------------------------
    # Event Handlers
    # ------------------------------------------------------------------
    def inspect_suggestion(self, payload: Dict[str, str]) -> Dict[str, any]:
        """Analyze an incoming LLM suggestion and emit a risk judgment."""

        suggestion = payload.get("content", "")
        llm_id = payload.get("llm_id", str(uuid.uuid4()))

        if self.llm_backend:
            suggestion = self.llm_backend(suggestion)

        red_flags = self._detect_risks(suggestion)
        judgment = self._build_judgment(llm_id, suggestion, red_flags)

        if self.meta_validator:
            meta_res = self.meta_validator.intercept_llm(
                {"text": suggestion, "llm_id": llm_id}
            )
            judgment["meta_feedback"] = meta_res

        self.send("LLM_EVALUATION_RESULT", judgment)
        return judgment

    def propose_fix(self, payload: Dict[str, str]) -> Dict[str, any]:
        """Return a basic safe patch for a reported issue."""

        issue = payload.get("issue", "Unclear bug")
        context = payload.get("context", "")

        if self.llm_backend:
            prompt = (
                "Provide a safe patch for the following issue:" f" {issue}\n{context}"
            )
            fix_code = self.llm_backend(prompt)
        else:
            fix_code = (
                f"# Auto-generated patch to address: {issue}\n"
                'print("[Fix applied]")\n'
                f"# Context: {context[:60]}"
            )

        review_result = None
        if self.patch_reviewer:
            review_result = self.patch_reviewer.review(fix_code)

        return {
            "patch": fix_code,
            "comment": "Proposed safe fix generated by GuardianInterceptor.",
            "confidence": 0.85,
            "next_step": "Await confirmation or peer review.",
            "patch_review": review_result,
        }

    # ------------------------------------------------------------------
    # Helper utilities
    # ------------------------------------------------------------------
    def _detect_risks(self, suggestion: str) -> List[str]:
        """Return a list of descriptive flags for risky content."""

        text = suggestion.lower()
        return [label for label, rule in RISK_PATTERNS.items() if rule(text)]

    @staticmethod
    def _build_judgment(llm_id: str, suggestion: str, flags: List[str]) -> Dict[str, any]:
        """Create the judgment structure returned to the message bus."""

        return {
            "llm_id": llm_id,
            "content_preview": suggestion[:60],
            "risk_level": "HIGH" if flags else "LOW",
            "flags": flags,
            "approved": not flags,
        }


# Example usage:
# guardian = GuardianInterceptorAgent()
# guardian.send("LLM_INCOMING", {"content": "delete everything in /tmp"})
